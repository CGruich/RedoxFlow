{
  "data": {
    "edges": [
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-70MFi",
            "name": "message",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "params_json",
            "id": "CustomComponent-41DIP",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "xy-edge__ChatInput-70MFi{œdataTypeœ:œChatInputœ,œidœ:œChatInput-70MFiœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-41DIP{œfieldNameœ:œparams_jsonœ,œidœ:œCustomComponent-41DIPœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}",
        "selected": false,
        "source": "ChatInput-70MFi",
        "sourceHandle": "{œdataTypeœ:œChatInputœ,œidœ:œChatInput-70MFiœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}",
        "target": "CustomComponent-41DIP",
        "targetHandle": "{œfieldNameœ:œparams_jsonœ,œidœ:œCustomComponent-41DIPœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"
      },
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "ChemPipeline",
            "id": "CustomComponent-41DIP",
            "name": "result",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-45RHL",
            "inputTypes": [
              "Data",
              "DataFrame",
              "Message"
            ],
            "type": "other"
          }
        },
        "id": "xy-edge__CustomComponent-41DIP{œdataTypeœ:œChemPipelineœ,œidœ:œCustomComponent-41DIPœ,œnameœ:œresultœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-45RHL{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-45RHLœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}",
        "selected": false,
        "source": "CustomComponent-41DIP",
        "sourceHandle": "{œdataTypeœ:œChemPipelineœ,œidœ:œCustomComponent-41DIPœ,œnameœ:œresultœ,œoutput_typesœ:[œMessageœ]}",
        "target": "ChatOutput-45RHL",
        "targetHandle": "{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-45RHLœ,œinputTypesœ:[œDataœ,œDataFrameœ,œMessageœ],œtypeœ:œotherœ}"
      }
    ],
    "nodes": [
      {
        "data": {
          "id": "CustomComponent-41DIP",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Transformers-only SMILES generator with constraints, memory, optional RDKit sanitize, Python/Bash hooks. Uses MoLFormer-recommended HF pattern.",
            "display_name": "ChemPipeline",
            "documentation": "",
            "edited": true,
            "field_order": [
              "params_json",
              "count",
              "max_atoms",
              "min_unique_elements",
              "elements",
              "model_id",
              "tokenizer_id",
              "temperature",
              "top_p",
              "top_k",
              "repetition_penalty",
              "num_return_sequences",
              "max_length",
              "device",
              "rdkit_filter",
              "remap_disallowed",
              "python_script",
              "python_args",
              "bash_cmd",
              "memory_path",
              "return_smiles"
            ],
            "frozen": false,
            "icon": "FlaskConical",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": false,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Result",
                "group_outputs": false,
                "hidden": null,
                "method": "run_pipeline",
                "name": "result",
                "options": null,
                "required_inputs": null,
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "bash_cmd": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Bash command template",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "bash_cmd",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from __future__ import annotations\nimport os, sys, json, time, subprocess, textwrap, re, csv, shlex\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Set, Optional, Tuple\n\nfrom langflow.custom import Component\nfrom langflow.io import (\n    MessageTextInput, StrInput, IntInput, DropdownInput,\n    Output, BoolInput, FloatInput\n)\nfrom langflow.schema import Message\n\n# RDKit is required for validation/sanitization\nfrom rdkit import Chem\nfrom rdkit.Chem import rdchem\nfrom rdkit import RDLogger\nRDLogger.DisableLog(\"rdApp.*\")\n\nMIN_ATOMS_FIXED = 4  # fixed minimum heavy atoms per request\n\nfrom time import sleep\n\nclass ChemPipeline(Component):\n    display_name = \"ChemPipeline\"\n    description = \"Transformers-only SMILES generator with constraints, memory, optional RDKit sanitize, Python/Bash hooks. Uses MoLFormer-recommended HF pattern.\"\n    icon = \"FlaskConical\"\n    name = \"ChemPipeline\"\n\n    # -------- Inputs ----------\n    inputs = [\n        MessageTextInput(\n            name=\"params_json\",\n            display_name=\"Params JSON\",\n            info=textwrap.dedent(\"\"\"\\\n                Optional JSON to override fields at runtime. Example:\n                {\n                  \"count\": 5,\n                  \"max_atoms\": 12,\n                  \"min_unique_elements\": 0,     // distinct NON-C elements required (C doesn't count)\n                  \"elements\": \"CNOF\",\n\n                  \"model_id\": \"ibm-research/GP-MoLFormer-Uniq\",\n                  \"tokenizer_id\": \"ibm-research/MoLFormer-XL-both-10pct\",\n\n                  \"temperature\": 0.7,\n                  \"top_p\": 0.9,\n                  \"top_k\": null,                 // null => omitted\n                  \"repetition_penalty\": 1.08,\n                  \"num_return_sequences\": 96,\n\n                  \"max_length\": 202,             // per model card\n\n                  \"device\": \"cuda:0\",\n                  \"rdkit_filter\": true,\n                  \"remap_disallowed\": true,\n\n                  // EITHER: legacy single-script form\n                  \"python_script\": \"hello_world.py\",\n                  \"python_args\": \"\",\n\n                  // OR: multi-script form (runs in order, after molecule generation)\n                  \"python_jobs\": [\n                    {\"script\": \"first.py\",  \"args\": \"--in data.csv --out step1.csv\"},\n                    {\"script\": \"second.py\", \"args\": [\"--in\", \"step1.csv\", \"--flag\", \"1\"]},\n                    {\"script\": \"third.py\",  \"args\": {\"--in\": \"step1.csv\", \"--out\": \"step3.csv\"}}\n                  ],\n\n                  \"bash_cmd\": \"echo {molecule}\",\n                  \"memory_path\": \"chem_memory.csv\",\n                  \"return_smiles\": true\n                }\n            \"\"\").strip(),\n            value='{\"count\": 5, \"max_atoms\": 10, \"min_unique_elements\": 1, \"elements\": \"CNOF\"}',\n        ),\n\n        # Targets / constraints\n        IntInput(name=\"count\", display_name=\"Count\", value=5),\n        IntInput(name=\"max_atoms\", display_name=\"Max heavy atoms\", value=10),\n        IntInput(name=\"min_unique_elements\", display_name=\"Min unique non-carbon elements\", value=1),\n        StrInput(name=\"elements\", display_name=\"Allowed elements\", value=\"CNOF\"),\n\n        # Transformers & decoding (MoLFormer card pairing)\n        StrInput(name=\"model_id\", display_name=\"HF Model ID\", value=\"ibm-research/GP-MoLFormer-Uniq\"),\n        StrInput(name=\"tokenizer_id\", display_name=\"HF Tokenizer ID\", value=\"ibm-research/MoLFormer-XL-both-10pct\"),\n\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.7),\n        FloatInput(name=\"top_p\", display_name=\"Top-p (nucleus)\", value=0.9),\n        IntInput(name=\"top_k\", display_name=\"Top-k (-1 => None)\", value=-1),\n        FloatInput(name=\"repetition_penalty\", display_name=\"Repetition penalty\", value=1.08),\n        IntInput(name=\"num_return_sequences\", display_name=\"# samples per round\", value=96),\n\n        # The card uses max_length=202.\n        IntInput(name=\"max_length\", display_name=\"Max length (card=202)\", value=202),\n\n        # Accelerator\n        DropdownInput(\n            name=\"device\",\n            display_name=\"Device\",\n            options=[\"auto\", \"cpu\", \"cuda:0\", \"cuda:1\", \"mps\"],\n            value=\"auto\",\n        ),\n\n        # Optional chemistry gate\n        BoolInput(name=\"rdkit_filter\", display_name=\"Filter by RDKit sanitize()\", value=True),\n        BoolInput(name=\"remap_disallowed\", display_name=\"Remap disallowed atoms to allowed set before filtering\", value=True),\n\n        # Hooks & memory\n        StrInput(name=\"python_script\", display_name=\"Python script path\", value=\"hello_world.py\"),\n        StrInput(name=\"python_args\", display_name=\"Python args\", value=\"\"),\n        StrInput(name=\"bash_cmd\", display_name=\"Bash command template\", value=\"\"),\n        StrInput(name=\"memory_path\", display_name=\"Memory file (csv)\", value=\"chem_memory.csv\"),\n        BoolInput(name=\"return_smiles\", display_name=\"Also return SMILES (echoed)\", value=True),\n    ]\n\n    outputs = [Output(display_name=\"Result\", name=\"result\", method=\"run_pipeline\")]\n\n    # Cached bundle: (model, tokenizer, model_id, tokenizer_id, device_str, dbg_ids)\n    _bundle: Optional[Tuple[Any, Any, str, str, str, Dict[str, int]]] = None\n\n    # Last filter stats (for debugging)\n    _last_filter_stats: Dict[str, int] = {}\n\n    # ---------------- Utilities ----------------\n    def _merge_params(self) -> Dict[str, Any]:\n        p = {\n            \"count\": int(self.count),\n            \"max_atoms\": int(self.max_atoms),\n            \"min_unique_elements\": max(0, int(getattr(self, \"min_unique_elements\", 1))),  # non-C uniques\n            \"elements\": \"\".join(sorted(set((self.elements or \"CNOF\").upper()))),\n\n            \"model_id\": (self.model_id or \"ibm-research/GP-MoLFormer-Uniq\").strip(),\n            \"tokenizer_id\": (self.tokenizer_id or \"ibm-research/MoLFormer-XL-both-10pct\").strip(),\n\n            \"temperature\": float(self.temperature) if str(self.temperature) else 0.7,\n            \"top_p\": float(getattr(self, \"top_p\", 0.9)),\n            \"top_k\": getattr(self, \"top_k\", -1),\n            \"repetition_penalty\": float(getattr(self, \"repetition_penalty\", 1.08)),\n            \"num_return_sequences\": max(1, int(getattr(self, \"num_return_sequences\", 96))),\n\n            \"max_length\": max(1, int(getattr(self, \"max_length\", 202))),\n\n            \"device\": str(getattr(self, \"device\", \"auto\")).strip().lower(),\n            \"rdkit_filter\": bool(getattr(self, \"rdkit_filter\", True)),\n            \"remap_disallowed\": bool(getattr(self, \"remap_disallowed\", True)),\n\n            # Legacy single-script fields (kept for backward compatibility)\n            \"python_script\": self.python_script or \"\",\n            \"python_args\": self.python_args or \"\",\n\n            # NEW: multi-script list (params_json only)\n            \"python_jobs\": [],\n\n            \"bash_cmd\": self.bash_cmd or \"\",\n            \"memory_path\": self.memory_path or \"chem_memory.csv\",\n            \"return_smiles\": bool(self.return_smiles),\n        }\n        try:\n            if self.params_json:\n                override = json.loads(str(self.params_json))\n                for k, v in override.items():\n                    # Ignore user-provided min_atoms and max_new_tokens (auto/fixed now)\n                    if k in (\"min_atoms\", \"max_new_tokens\"):\n                        continue\n                    p[k] = v\n        except Exception:\n            pass\n\n        # Normalize\n        p[\"count\"] = int(p[\"count\"])\n        p[\"max_atoms\"] = int(p[\"max_atoms\"])\n        p[\"min_unique_elements\"] = max(0, int(p[\"min_unique_elements\"]))\n        p[\"temperature\"] = float(p[\"temperature\"])\n        p[\"top_p\"] = float(p[\"top_p\"])\n        # Allow None-like top_k via JSON null, or -1 via UI\n        if p[\"top_k\"] in (None, \"None\"):\n            p[\"top_k\"] = None\n        else:\n            try:\n                tk = int(p[\"top_k\"])\n                p[\"top_k\"] = None if tk < 0 else tk\n            except Exception:\n                p[\"top_k\"] = None\n        p[\"repetition_penalty\"] = float(p[\"repetition_penalty\"])\n        p[\"num_return_sequences\"] = max(1, int(p[\"num_return_sequences\"]))\n        p[\"max_length\"] = max(1, int(p[\"max_length\"]))\n        p[\"device\"] = str(p[\"device\"]).strip().lower()\n        return p\n\n    def _resolve_device(self, device_str: str) -> str:\n        try:\n            import torch\n        except Exception:\n            return \"cpu\"\n        s = (device_str or \"auto\").lower().strip()\n        if s == \"auto\":\n            if torch.cuda.is_available():\n                return \"cuda:0\"\n            if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n                return \"mps\"\n            return \"cpu\"\n        if s == \"cpu\":\n            return \"cpu\"\n        if s == \"mps\":\n            return \"mps\" if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available() else \"cpu\"\n        if s.startswith(\"cuda\"):\n            if not torch.cuda.is_available():\n                return \"cpu\"\n            try:\n                _ = int(s.split(\":\")[1]) if \":\" in s else 0\n            except Exception:\n                s = \"cuda:0\"\n            return s\n        return \"cpu\"\n\n    # ---- Load model/tokenizer and robustly ensure PAD/EOS/BOS ----\n    def _get_bundle(self, model_id: str, tokenizer_id: str, device_choice: str):\n        device_str = self._resolve_device(device_choice)\n        if self._bundle:\n            mdl, tok, mid, tid, dev, _ = self._bundle\n            if mid == model_id and tid == tokenizer_id and dev == device_str:\n                return self._bundle\n\n        from transformers import AutoTokenizer, AutoModelForCausalLM\n        import torch\n\n        tok = AutoTokenizer.from_pretrained(tokenizer_id, trust_remote_code=True)\n\n        added = False\n        # Ensure EOS\n        if tok.eos_token_id is None:\n            # prefer sep if present; else add a safe EOS\n            if getattr(tok, \"sep_token\", None) is not None:\n                tok.eos_token = tok.sep_token\n            else:\n                tok.add_special_tokens({\"eos_token\": \"</s>\"})\n                added = True\n        # Ensure PAD\n        if tok.pad_token_id is None:\n            # if eos exists, reuse it; else add dedicated PAD\n            if tok.eos_token_id is not None:\n                tok.pad_token = tok.eos_token\n            else:\n                tok.add_special_tokens({\"pad_token\": \"[PAD]\"})\n                added = True\n        # Ensure BOS\n        if getattr(tok, \"bos_token_id\", None) is None:\n            # prefer cls if present; else add a safe BOS\n            if getattr(tok, \"cls_token\", None) is not None:\n                tok.bos_token = tok.cls_token\n            else:\n                tok.add_special_tokens({\"bos_token\": \"<s>\"})\n                added = True\n\n        mdl = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n        if added:\n            mdl.resize_token_embeddings(len(tok))\n\n        # Write into config so generation doesn't need explicit kwargs\n        if getattr(mdl.config, \"pad_token_id\", None) is None:\n            mdl.config.pad_token_id = tok.pad_token_id\n        if getattr(mdl.config, \"eos_token_id\", None) is None:\n            mdl.config.eos_token_id = tok.eos_token_id\n\n        mdl.to(device_str)\n\n        dbg_ids = dict(pad=tok.pad_token_id or -1,\n                       eos=tok.eos_token_id or -1,\n                       bos=getattr(tok, \"bos_token_id\", None) or -1)\n\n        self._bundle = (mdl, tok, model_id, tokenizer_id, device_str, dbg_ids)\n        return self._bundle\n\n    # ---------- Text generation (MoLFormer-card style) ----------\n    def _generate_texts(self, p: Dict[str, Any], n_samples: int) -> List[str]:\n        \"\"\"\n        Promptless sampling by default (BOS token). Mirrors the model card behavior.\n        \"\"\"\n        import torch\n        mdl, tok, _, _, device_str, _ = self._get_bundle(p[\"model_id\"], p[\"tokenizer_id\"], p[\"device\"])\n\n        # Build a BOS-only prompt\n        bos_id = getattr(tok, \"bos_token_id\", None)\n        if bos_id is None:\n            bos_id = tok.eos_token_id if tok.eos_token_id is not None else tok.pad_token_id\n        if bos_id is None:\n            bos_id = 0  # absolute fallback; won't be used if specials were set above\n\n        device = torch.device(device_str)\n        input_ids = torch.tensor([[bos_id]], dtype=torch.long, device=device)\n\n        gen_kwargs = dict(\n            do_sample=True,\n            num_return_sequences=max(1, int(n_samples)),\n            pad_token_id=tok.pad_token_id,\n            eos_token_id=tok.eos_token_id,\n        )\n\n        # Auto-determine max_new_tokens = 1.5 * max_atoms (rounded)\n        auto_mnt = max(1, int(round(1.5 * p[\"max_atoms\"])))\n        gen_kwargs[\"max_new_tokens\"] = auto_mnt\n\n        # Decoding controls\n        if p[\"temperature\"] is not None:\n            gen_kwargs[\"temperature\"] = float(p[\"temperature\"])\n        if p[\"top_p\"] is not None:\n            gen_kwargs[\"top_p\"] = float(p[\"top_p\"])\n        if p[\"top_k\"] is not None:  # only include if not None; avoids TypeError\n            gen_kwargs[\"top_k\"] = int(p[\"top_k\"])\n        if p[\"repetition_penalty\"] is not None:\n            gen_kwargs[\"repetition_penalty\"] = float(p[\"repetition_penalty\"])\n\n        with torch.inference_mode():\n            outputs = mdl.generate(input_ids=input_ids, **gen_kwargs)\n\n        texts = tok.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        return [t for t in texts if t and t.strip()]\n\n    # ---------- SMILES utilities ----------\n    _smiles_line_re = re.compile(r\"^[A-Za-z0-9@+\\-\\[\\]\\(\\)=#\\\\/\\.%]+$\")\n\n    def _extract_smiles_candidates(self, text: str) -> List[str]:\n        if not text:\n            return []\n        text = text.replace(\"```\", \" \").replace(\"`\", \" \")\n        raw = []\n        for line in re.split(r\"[\\n;]\", text):\n            line = line.strip()\n            if not line:\n                continue\n            # remove all spaces inside, MoLFormer often emits plain SMILES without spaces anyway\n            line = re.sub(r\"\\s+\", \"\", line)\n            if self._smiles_line_re.match(line):\n                raw.append(line)\n        seen, out = set(), []\n        for s in raw:\n            if s not in seen:\n                seen.add(s)\n                out.append(s)\n        return out\n\n    def _elements_in_mol(self, mol) -> Set[str]:\n        return {a.GetSymbol() for a in mol.GetAtoms()}\n\n    def _remap_disallowed_mol(self, mol, allowed: Set[str]) -> rdchem.Mol | None:\n        mapping = {\n            \"Cl\": \"F\", \"Br\": \"F\", \"I\": \"F\",\n            \"S\": \"O\",\n            \"P\": \"N\",\n            \"Si\": \"C\", \"Sn\": \"C\", \"B\": \"C\",\n        }\n        try:\n            for atom in mol.GetAtoms():\n                sym = atom.GetSymbol()\n                if sym not in allowed:\n                    if sym in mapping and mapping[sym] in allowed:\n                        new_sym = mapping[sym]\n                        atom.SetAtomicNum(Chem.GetPeriodicTable().GetAtomicNumber(new_sym))\n                    else:\n                        atom.SetAtomicNum(0)  # mark for removal\n            to_remove = [a.GetIdx() for a in mol.GetAtoms() if a.GetAtomicNum() == 0]\n            to_remove.sort(reverse=True)\n            em = Chem.EditableMol(mol)\n            for idx in to_remove:\n                em.RemoveAtom(idx)\n            new_mol = em.GetMol()\n            Chem.SanitizeMol(new_mol)\n            return new_mol\n        except Exception:\n            return None\n\n    # ---- Ring-strain heuristic: filter unrealistic rings / highly strained fused small rings ----\n    def _passes_ring_strain(self, mol: rdchem.Mol) -> bool:\n        \"\"\"\n        Conservative heuristic:\n          - Reject any ring of size < 3.\n          - For 3-membered rings: reject if any DOUBLE/TRIPLE bond in the ring or any sp atom.\n          - For 4-membered rings: reject if any TRIPLE bond or >=2 DOUBLE bonds in the ring, or any sp atom.\n          - Reject fused small-ring systems where an atom participates in >=2 rings of size <= 4 (e.g., bicyclobutane-like).\n        Allows common small rings like cyclopropane/cyclobutane when saturated and not multiply fused.\n        \"\"\"\n        try:\n            ri = mol.GetRingInfo()\n            atom_rings = list(ri.AtomRings())\n            bond_rings = list(ri.BondRings())\n\n            # Per-ring checks\n            for i, atoms in enumerate(atom_rings):\n                n = len(atoms)\n                if n < 3:\n                    return False  # impossible ring sizes\n                bonds = bond_rings[i] if i < len(bond_rings) else []\n                num_double = 0\n                num_triple = 0\n                for bidx in bonds:\n                    bt = mol.GetBondWithIdx(bidx).GetBondType()\n                    if bt == rdchem.BondType.DOUBLE:\n                        num_double += 1\n                    elif bt == rdchem.BondType.TRIPLE:\n                        num_triple += 1\n                sp_in_ring = any(\n                    mol.GetAtomWithIdx(a).GetHybridization() == rdchem.HybridizationType.SP\n                    for a in atoms\n                )\n\n                if n == 3:\n                    if num_double > 0 or num_triple > 0:\n                        return False\n                    if sp_in_ring:\n                        return False\n                elif n == 4:\n                    if num_triple > 0:\n                        return False\n                    if num_double >= 2:\n                        return False\n                    if sp_in_ring:\n                        return False\n\n            # Fused small-ring check (atoms shared by >=2 rings of size <=4)\n            ring_sizes_per_atom: Dict[int, List[int]] = {i: [] for i in range(mol.GetNumAtoms())}\n            for atoms in atom_rings:\n                n = len(atoms)\n                for a in atoms:\n                    ring_sizes_per_atom[a].append(n)\n            for sizes in ring_sizes_per_atom.values():\n                smalls = [s for s in sizes if s <= 4]\n                if len(smalls) >= 2:\n                    return False\n\n            return True\n        except Exception:\n            # If analysis fails, be conservative and reject\n            return False\n\n    # ---- Unrealistic-moieties heuristic: ban specific, physically dubious substructures ----\n    def _fails_unrealistic_motifs(self, mol: rdchem.Mol) -> bool:\n        \"\"\"\n        Conservative set of red flags:\n          - O–F and N–F bonds (highly unstable in ordinary organic contexts):  [O]-[F], [N]-[F]\n          - F engaged in multiple bonds:                                      [F]=*, [F]#*\n          - O=O (dioxygen-like embedded or as a fragment):                    [O]=[O]\n          - Halogen–halogen F–F single bond:                                  [F]-[F]\n          - Dinitrogen triple bond in structure:                              [N]#[N]\n          - Carbon–fluorine multiple bonds:                                   [#6]=[F], [#6]#[F]\n          - O–O bond inside a very small ring (size ≤ 4)\n        This list is intentionally narrow and extendable.\n        \"\"\"\n        try:\n            patterns = getattr(self, \"_forbidden_queries\", None)\n            if patterns is None:\n                pats = [\n                    (\"O-F\", Chem.MolFromSmarts(\"[O]-[F]\")),\n                    (\"N-F\", Chem.MolFromSmarts(\"[N]-[F]\")),\n                    (\"F=*\", Chem.MolFromSmarts(\"[F]=*\")),\n                    (\"F#*\", Chem.MolFromSmarts(\"[F]#*\")),\n                    (\"O=O\", Chem.MolFromSmarts(\"[O]=[O]\")),\n                    (\"F-F\", Chem.MolFromSmarts(\"[F]-[F]\")),\n                    (\"N#N\", Chem.MolFromSmarts(\"[N]#[N]\")),\n                    (\"C=F\", Chem.MolFromSmarts(\"[#6]=[F]\")),\n                    (\"C#F\", Chem.MolFromSmarts(\"[#6]#[F]\")),\n                ]\n                # Filter out any None in case a SMARTS fails to parse\n                patterns = [(name, q) for (name, q) in pats if q is not None]\n                self._forbidden_queries = patterns\n\n            # SMARTS screen\n            for _, q in patterns:\n                if mol.HasSubstructMatch(q):\n                    return True  # a forbidden motif is present\n        except Exception:\n            # On any error, be conservative and DON'T fail here; ring/valence checks still apply.\n            return False\n\n        # Special-case: O–O bond inside very small rings (≤ 4)\n        try:\n            ri = mol.GetRingInfo()\n            bond_rings = list(ri.BondRings())\n            for b in mol.GetBonds():\n                a1, a2 = b.GetBeginAtom(), b.GetEndAtom()\n                if a1.GetSymbol() == \"O\" and a2.GetSymbol() == \"O\":\n                    if b.IsInRing():\n                        # Check all rings containing this bond; if any ring size <= 4, reject\n                        bidx = b.GetIdx()\n                        for ring in bond_rings:\n                            if bidx in ring and len(ring) <= 4:\n                                return True\n        except Exception:\n            pass\n\n        return False  # no unrealistic motifs found\n\n    def _enforce_constraints_smiles(\n        self,\n        smiles_list: List[str],\n        max_atoms: int,\n        elements: str,\n        min_atoms: int = 1,\n        min_unique_nonC: int = 0,\n        rdkit_filter: bool = True,\n        remap_disallowed: bool = True,\n    ) -> List[str]:\n        stats = {\n            \"too_few_atoms\": 0,\n            \"too_many_atoms\": 0,\n            \"not_enough_unique_nonC\": 0,\n            \"rdkit_failed\": 0,\n            \"has_disallowed_atom\": 0,\n            \"ring_strain_failed\": 0,  # NEW: filtered by ring-strain heuristic\n            \"unrealistic_moiety\": 0,  # NEW: filtered by SMARTS-based unrealistic motifs\n        }\n        allowed = set(elements)\n        keep: List[str] = []\n\n        for s in smiles_list:\n            if not s:\n                stats[\"rdkit_failed\"] += 1\n                continue\n            try:\n                mol = Chem.MolFromSmiles(s, sanitize=True)\n                if mol is None:\n                    stats[\"rdkit_failed\"] += 1\n                    continue\n            except Exception:\n                stats[\"rdkit_failed\"] += 1\n                continue\n\n            mol_work = mol\n            elems = self._elements_in_mol(mol_work)\n            if any(e not in allowed for e in elems):\n                if not remap_disallowed:\n                    stats[\"has_disallowed_atom\"] += 1\n                    continue\n                mol_work = self._remap_disallowed_mol(mol_work, allowed)\n                if mol_work is None:\n                    stats[\"rdkit_failed\"] += 1\n                    continue\n                elems = self._elements_in_mol(mol_work)\n                if any(e not in allowed for e in elems):\n                    stats[\"has_disallowed_atom\"] += 1\n                    continue\n\n            n_heavy = mol_work.GetNumHeavyAtoms()\n            if n_heavy < min_atoms:\n                stats[\"too_few_atoms\"] += 1\n                continue\n            if n_heavy > max_atoms:\n                stats[\"too_many_atoms\"] += 1\n                continue\n\n            nonC = {e for e in elems if e != \"C\"}\n            if len(nonC) < min_unique_nonC:\n                stats[\"not_enough_unique_nonC\"] += 1\n                continue\n\n            # Ring-strain heuristic filter\n            if not self._passes_ring_strain(mol_work):\n                stats[\"ring_strain_failed\"] += 1\n                continue\n\n            # Unrealistic moieties filter\n            if self._fails_unrealistic_motifs(mol_work):\n                stats[\"unrealistic_moiety\"] += 1\n                continue\n\n            try:\n                can = Chem.MolToSmiles(mol_work, canonical=True)\n            except Exception:\n                stats[\"rdkit_failed\"] += 1\n                continue\n            keep.append(can)\n\n        self._last_filter_stats = stats\n        return keep\n\n    # ---------- Memory ----------\n    def _load_memory(self, path: str) -> Set[str]:\n        tried: Set[str] = set()\n        p = Path(path)\n        if not p.exists():\n            return tried\n\n        if p.suffix.lower() == \".csv\":\n            try:\n                with p.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n                    reader = csv.DictReader(f)\n                    for row in reader:\n                        s = (row.get(\"smiles\") or \"\").strip()\n                        if s:\n                            tried.add(s)\n            except Exception:\n                # fallback: read raw lines skipping header if present\n                with p.open(\"r\", encoding=\"utf-8\") as f:\n                    for i, line in enumerate(f):\n                        if i == 0 and \"smiles\" in line.lower():\n                            continue\n                        parts = [x.strip() for x in line.split(\",\")]\n                        if len(parts) >= 2 and parts[1]:\n                            tried.add(parts[1])\n            return tried\n\n    def _ensure_memory_file(self, path: str) -> None:\n        p = Path(path)\n        p.parent.mkdir(parents=True, exist_ok=True)\n        if not p.exists() or p.stat().st_size == 0:\n            if p.suffix.lower() == \".csv\":\n                with p.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n                    writer = csv.writer(f)\n                    writer.writerow([\"index\", \"smiles\"])\n            else:\n                p.touch(exist_ok=True)\n\n    def _get_next_memory_index(self, path: str) -> int:\n        \"\"\"\n        Scan memory file and return the next integer index.\n        CSV: read 'index' column; JSONL: read integer 'smiles' field if present.\n        \"\"\"\n        p = Path(path)\n        if not p.exists() or p.stat().st_size == 0:\n            return 1\n\n        max_idx = 0\n        if p.suffix.lower() == \".csv\":\n            try:\n                with p.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n                    reader = csv.DictReader(f)\n                    for row in reader:\n                        try:\n                            idx = int(row.get(\"index\", \"0\"))\n                            if idx > max_idx:\n                                max_idx = idx\n                        except Exception:\n                            continue\n                return max_idx + 1 if max_idx >= 0 else 1\n            except Exception:\n                return 1\n\n    def _append_memory(self, path: str, smiles_list: List[str]) -> None:\n        p = Path(path)\n        p.parent.mkdir(parents=True, exist_ok=True)\n        next_idx = self._get_next_memory_index(path)\n\n        if p.suffix.lower() == \".csv\":\n            # Ensure header exists\n            self._ensure_memory_file(path)\n            with p.open(\"a\", encoding=\"utf-8\", newline=\"\") as f:\n                writer = csv.writer(f)\n                for s in smiles_list:\n                    writer.writerow([next_idx, s])\n                    next_idx += 1\n            return\n\n    # ---------- Arg helpers to handle spaces in paths ----------\n    def _split_args_allow_spaces(self, argstr: str) -> List[str]:\n        \"\"\"\n        Robustly split an unquoted argument string where values (paths) may contain spaces.\n        Heuristic: treat any token following a flag (starts with '-') as part of its value\n        until the next token that starts with '-' (a new flag).\n        \"\"\"\n        if not argstr:\n            return []\n        raw = argstr.strip().split()\n        out: List[str] = []\n        i = 0\n        while i < len(raw):\n            tok = raw[i]\n            if tok.startswith(\"-\"):\n                out.append(tok)\n                i += 1\n                # gather value(s) until next flag or end\n                start = i\n                while i < len(raw) and not raw[i].startswith(\"-\"):\n                    i += 1\n                if i > start:\n                    out.append(\" \".join(raw[start:i]))\n            else:\n                # standalone (unlikely), keep as-is\n                out.append(tok)\n                i += 1\n        return out\n\n    def _normalize_python_args(self, args: Any) -> List[str]:\n        \"\"\"\n        Accepts:\n          - str: e.g. \"--molecules /path with space/file.csv --depth 2\"\n          - dict: {\"--molecules\": \"/path with space/file.csv\", \"--depth\": 2, \"--out\": \"/x y/z.csv\"}\n          - list: [\"--molecules\", \"/path with space/file.csv\", \"--depth\", \"2\"]\n        Returns a list suitable for subprocess.run([...]).\n        \"\"\"\n        if args is None or args == \"\":\n            return []\n        # dict preserves spaces without extra quoting; each value is its own argv element\n        if isinstance(args, dict):\n            out: List[str] = []\n            for k, v in args.items():\n                out.append(str(k))\n                if isinstance(v, (list, tuple)):\n                    out.extend([str(vi) for vi in v])\n                else:\n                    out.append(str(v))\n            return out\n        # list: pass through, casting to str\n        if isinstance(args, list):\n            return [str(x) for x in args]\n        # str: try shlex first (handles already-quoted inputs); if it doesn't look quoted and\n        # contains spaces, fall back to the heuristic splitter to preserve spaced paths.\n        if isinstance(args, str):\n            # If it contains any quotes, assume user quoted properly and trust shlex.split\n            if any(q in args for q in ['\"', \"'\"]):\n                return shlex.split(args)\n            # No quotes present: use heuristic grouping by flags\n            return self._split_args_allow_spaces(args)\n        # fallback\n        return [str(args)]\n\n    # ---------- NEW: normalize multiple python jobs ----------\n    def _coerce_python_jobs(self, jobs: Any, single_script: str, single_args: Any) -> List[Dict[str, Any]]:\n        \"\"\"\n        Accepts:\n          - jobs: None or list of jobs; each job may be:\n              * {\"script\": \"...\", \"args\": <str|list|dict|None>}\n              * \"path/to/script.py\"  (args = None)\n          - single_script/single_args: legacy fields; used if 'jobs' is empty.\n        Returns: [{\"script\": str, \"args\": Any}, ...] (order preserved)\n        \"\"\"\n        out: List[Dict[str, Any]] = []\n        if isinstance(jobs, list) and jobs:\n            for j in jobs:\n                if isinstance(j, str):\n                    s = j.strip()\n                    if s:\n                        out.append({\"script\": s, \"args\": None})\n                elif isinstance(j, dict):\n                    s = str(j.get(\"script\", \"\")).strip()\n                    if not s:\n                        continue\n                    out.append({\"script\": s, \"args\": j.get(\"args\", None)})\n            if out:\n                return out\n\n        # Fallback to the legacy single-script fields\n        if single_script:\n            out.append({\"script\": single_script, \"args\": single_args})\n        return out\n\n    # ---------- Hooks ----------\n    def _run_python(self, script: str, args: Any, molecule: str) -> Dict[str, Any]:\n        if not script:\n            return {}\n        env = os.environ.copy()\n        env[\"MOLECULE\"] = molecule\n        argv = [sys.executable, script] + self._normalize_python_args(args)\n        try:\n            proc = subprocess.run(argv, capture_output=True, text=True, env=env, timeout=300)\n            return {\n                \"rc\": proc.returncode,\n                \"stdout\": proc.stdout,\n                \"stderr\": proc.stderr,\n                \"cmd\": shlex.join(argv),  # pretty/quoted for spaces\n            }\n        except Exception as e:\n            return {\"error\": str(e), \"cmd\": shlex.join(argv)}\n\n    def _run_python_once(self, script: str, args: Any, memory_path: str) -> Dict[str, Any]:\n        \"\"\"\n        Run an arbitrary Python script ONCE after generation.\n        Exposes MOLECULES_CSV=<memory_path> in the environment for convenience.\n        \"\"\"\n        if not script:\n            return {}\n        env = os.environ.copy()\n        env[\"MOLECULES_CSV\"] = str(memory_path)\n        argv = [sys.executable, script] + self._normalize_python_args(args)\n        try:\n            proc = subprocess.run(argv, capture_output=True, text=True, env=env, timeout=300)\n            return {\n                \"rc\": proc.returncode,\n                \"stdout\": proc.stdout,\n                \"stderr\": proc.stderr,\n                \"cmd\": shlex.join(argv),  # pretty/quoted for spaces\n            }\n        except Exception as e:\n            return {\"error\": str(e), \"cmd\": shlex.join(argv)}\n\n    # ---------- NEW: run multiple python scripts (sequentially) ----------\n    def _run_python_jobs_once(self, jobs: List[Dict[str, Any]], memory_path: str) -> List[Dict[str, Any]]:\n        \"\"\"\n        Runs each job in order. Each job is {\"script\": str, \"args\": Any}.\n        Returns a list of per-job run dicts (rc/stdout/stderr/cmd).\n        \"\"\"\n        runs: List[Dict[str, Any]] = []\n        for j in jobs:\n            script = j.get(\"script\") or \"\"\n            args = j.get(\"args\", None)\n            runs.append(self._run_python_once(script, args, memory_path))\n        return runs\n\n    def _run_bash(self, template: str, molecule: str, memory_path: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"\n        Per-molecule bash hook.\n\n        Supports placeholders:\n          {molecule}    -> raw molecule string (no quoting)\n          {molecule_q}  -> shell-quoted molecule (safe for spaces/specials)\n          {memory_path} -> raw memory path (no quoting)\n          {memory_path_q} -> shell-quoted memory path\n        \"\"\"\n        if not template:\n            return {}\n        # Safe substitutions\n        safe_mol = shlex.quote(molecule if molecule is not None else \"\")\n        safe_mem = shlex.quote(memory_path if memory_path is not None else \"\")\n        cmd = (\n            template\n            .replace(\"{molecule_q}\", safe_mol)\n            .replace(\"{molecule}\", molecule)\n            .replace(\"{memory_path_q}\", safe_mem)\n            .replace(\"{memory_path}\", memory_path or \"\")\n        )\n        \n        sleep(10)\n        \n        try:\n            proc = subprocess.run([\"/bin/bash\", \"-lc\", cmd], capture_output=True, text=True, timeout=300)\n            return {\"rc\": proc.returncode, \"stdout\": proc.stdout, \"stderr\": proc.stderr, \"cmd\": cmd}\n        except Exception as e:\n            return {\"error\": str(e), \"cmd\": cmd}\n\n    def _run_bash_once(self, template: str, memory_path: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"\n        Run a bash command ONCE at the very end (after Python script).\n    \n        Placeholders (same as _run_bash):\n          {molecule}       -> raw molecule string (unused here; replaced with \"\")\n          {molecule_q}     -> shell-quoted molecule (unused here; replaced with \"\")\n          {memory_path}    -> raw memory path (no quoting)\n          {memory_path_q}  -> shell-quoted memory path (safe for spaces/special chars)\n        \"\"\"\n        if not template:\n            return {}\n    \n        # Quote-sensitive replacements (just like _run_bash)\n        safe_mol = shlex.quote(\"\")  # no per-molecule context at the end run\n        safe_mem = shlex.quote(memory_path if memory_path is not None else \"\")\n    \n        cmd = (\n            template\n            .replace(\"{molecule_q}\", safe_mol)\n            .replace(\"{molecule}\", \"\")\n            .replace(\"{memory_path_q}\", safe_mem)\n            .replace(\"{memory_path}\", memory_path or \"\")\n        )\n        \n        sleep(10)\n        \n        try:\n            proc = subprocess.run([\"/bin/bash\", \"-lc\", cmd], capture_output=True, text=True, timeout=300)\n            return {\"rc\": proc.returncode, \"stdout\": proc.stdout, \"stderr\": proc.stderr, \"cmd\": cmd}\n        except Exception as e:\n            return {\"error\": str(e), \"cmd\": cmd}\n    \n\n    # ---------------- Main ----------------\n    def run_pipeline(self) -> Message:\n        p = self._merge_params()\n        self._ensure_memory_file(p[\"memory_path\"])\n\n        tried = self._load_memory(p[\"memory_path\"])\n        seen_this_run = set()\n        fresh: List[str] = []\n        raw_chunks: List[str] = []\n        rounds = 0\n        MAX_ROUNDS = 30\n\n        # Capture tokenizer special IDs for debug visibility\n        _, _, _, _, _, dbg_ids = self._get_bundle(p[\"model_id\"], p[\"tokenizer_id\"], p[\"device\"])\n\n        while len(fresh) < p[\"count\"] and rounds < MAX_ROUNDS:\n            rounds += 1\n\n            texts = self._generate_texts(p, n_samples=p[\"num_return_sequences\"])\n            raw_chunks.extend(texts)\n\n            candidates: List[str] = []\n            for t in texts:\n                candidates.extend(self._extract_smiles_candidates(t))\n\n            if candidates:\n                filtered = self._enforce_constraints_smiles(\n                    candidates,\n                    max_atoms=p[\"max_atoms\"],\n                    elements=p[\"elements\"],\n                    min_atoms=MIN_ATOMS_FIXED,\n                    min_unique_nonC=p[\"min_unique_elements\"],\n                    rdkit_filter=p[\"rdkit_filter\"],\n                    remap_disallowed=p[\"remap_disallowed\"],\n                )\n            else:\n                filtered = []\n\n            for s in filtered:\n                if s in tried or s in seen_this_run:\n                    continue\n                seen_this_run.add(s)\n                fresh.append(s)\n                if len(fresh) >= p[\"count\"]:\n                    break\n\n        fresh = fresh[: p[\"count\"]]\n        self._append_memory(p[\"memory_path\"], fresh)\n\n        # --- NEW: resolve multi-script vs legacy single-script\n        jobs = self._coerce_python_jobs(\n            p.get(\"python_jobs\", []),\n            p.get(\"python_script\", \"\"),\n            p.get(\"python_args\", \"\"),\n        )\n\n        # Run Python scripts ONCE at the very end (in order)\n        python_runs: List[Dict[str, Any]] = []\n        if jobs:\n            python_runs = self._run_python_jobs_once(jobs, p[\"memory_path\"])\n\n        # Run bash command ONCE at the very end (after Python scripts)\n        bash_run = {}\n        if p[\"bash_cmd\"]:\n            bash_run = self._run_bash_once(p[\"bash_cmd\"], memory_path=p[\"memory_path\"])\n\n        # Results payload (no per-molecule bash now)\n        results = [{\"smiles\": s} for s in fresh]\n\n        body = {\n            \"params_used\": {\n                k: v for k, v in p.items()\n                if k not in (\"model_id\", \"tokenizer_id\")\n            },\n            \"hf_special_token_ids\": dbg_ids,  # helpful to confirm pad/eos/bos were set\n            \"count_generated\": len(results),\n            \"filter_stats\": getattr(self, \"_last_filter_stats\", {}),\n            \"raw_samples\": len(raw_chunks),\n            \"raw_model_text_truncated\": (\"\\n\".join(raw_chunks))[:800] if raw_chunks else \"\",\n            # NEW: list of per-script runs; keep first item as 'python_run' for backward compat if length==1\n            \"python_runs\": python_runs,\n            \"python_run\": (python_runs[0] if len(python_runs) == 1 else None),\n            \"bash_run\": bash_run,      # single post-generation bash execution\n            \"results\": results\n        }\n        text = json.dumps(body, indent=2, ensure_ascii=False)\n        return Message(text=text)\n"
              },
              "count": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Count",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "count",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 10
              },
              "device": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Device",
                "dynamic": false,
                "info": "",
                "name": "device",
                "options": [
                  "auto",
                  "cpu",
                  "cuda:0",
                  "cuda:1",
                  "mps"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "auto"
              },
              "elements": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Allowed elements",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "elements",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "CNOF"
              },
              "max_atoms": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Max heavy atoms",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "max_atoms",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 16
              },
              "max_length": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Max length (card=202)",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "max_length",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 204
              },
              "memory_path": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Memory file (csv)",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "memory_path",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "reactants_memory.json"
              },
              "min_unique_elements": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Min unique non-carbon elements",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "min_unique_elements",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1
              },
              "model_id": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "HF Model ID",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "model_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "ibm-research/GP-MoLFormer-Uniq"
              },
              "num_return_sequences": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "# samples per round",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "num_return_sequences",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 96
              },
              "params_json": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "Params JSON",
                "dynamic": false,
                "info": "Optional JSON to override fields at runtime. Example:\n{\n  \"count\": 5,\n  \"max_atoms\": 12,\n  \"min_unique_elements\": 0,     // distinct NON-C elements required (C doesn't count)\n  \"elements\": \"CNOF\",\n\n  \"model_id\": \"ibm-research/GP-MoLFormer-Uniq\",\n  \"tokenizer_id\": \"ibm-research/MoLFormer-XL-both-10pct\",\n\n  \"temperature\": 0.7,\n  \"top_p\": 0.9,\n  \"top_k\": null,                 // null => omitted\n  \"repetition_penalty\": 1.08,\n  \"num_return_sequences\": 96,\n\n  \"max_length\": 202,             // per model card\n\n  \"device\": \"cuda:0\",\n  \"rdkit_filter\": true,\n  \"remap_disallowed\": true,\n\n  // EITHER: legacy single-script form\n  \"python_script\": \"hello_world.py\",\n  \"python_args\": \"\",\n\n  // OR: multi-script form (runs in order, after molecule generation)\n  \"python_jobs\": [\n    {\"script\": \"first.py\",  \"args\": \"--in data.csv --out step1.csv\"},\n    {\"script\": \"second.py\", \"args\": [\"--in\", \"step1.csv\", \"--flag\", \"1\"]},\n    {\"script\": \"third.py\",  \"args\": {\"--in\": \"step1.csv\", \"--out\": \"step3.csv\"}}\n  ],\n\n  \"bash_cmd\": \"echo {molecule}\",\n  \"memory_path\": \"chem_memory.csv\",\n  \"return_smiles\": true\n}",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "params_json",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "python_args": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Python args",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "python_args",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "python_script": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "Python script path",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "python_script",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "hello_world.py"
              },
              "rdkit_filter": {
                "_input_type": "BoolInput",
                "advanced": false,
                "display_name": "Filter by RDKit sanitize()",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "rdkit_filter",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "remap_disallowed": {
                "_input_type": "BoolInput",
                "advanced": false,
                "display_name": "Remap disallowed atoms to allowed set before filtering",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "remap_disallowed",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "repetition_penalty": {
                "_input_type": "FloatInput",
                "advanced": false,
                "display_name": "Repetition penalty",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "repetition_penalty",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 1.1
              },
              "return_smiles": {
                "_input_type": "BoolInput",
                "advanced": false,
                "display_name": "Also return SMILES (echoed)",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "return_smiles",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "temperature": {
                "_input_type": "FloatInput",
                "advanced": false,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "temperature",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.425
              },
              "tokenizer_id": {
                "_input_type": "StrInput",
                "advanced": false,
                "display_name": "HF Tokenizer ID",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "tokenizer_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "ibm-research/MoLFormer-XL-both-10pct"
              },
              "top_k": {
                "_input_type": "IntInput",
                "advanced": false,
                "display_name": "Top-k (-1 => None)",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "top_k",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": -1
              },
              "top_p": {
                "_input_type": "FloatInput",
                "advanced": false,
                "display_name": "Top-p (nucleus)",
                "dynamic": false,
                "info": "",
                "list": false,
                "list_add_label": "Add More",
                "name": "top_p",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.9
              }
            },
            "tool_mode": false
          },
          "showNode": true,
          "type": "ChemPipeline"
        },
        "dragging": false,
        "id": "CustomComponent-41DIP",
        "measured": {
          "height": 1780,
          "width": 320
        },
        "position": {
          "x": 685.1126039717382,
          "y": 548.055828635732
        },
        "selected": true,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ChatInput-70MFi",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Get chat inputs from the Playground.",
            "display_name": "Chat Input",
            "documentation": "https://docs.langflow.org/components-io#chat-input",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "files",
              "background_color",
              "chat_icon",
              "text_color"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Chat Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "background_color": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Background Color",
                "dynamic": false,
                "info": "The background color of the icon.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "background_color",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "chat_icon": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Icon",
                "dynamic": false,
                "info": "The icon of the message.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "chat_icon",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs.inputs import BoolInput\nfrom langflow.io import (\n    DropdownInput,\n    FileInput,\n    MessageTextInput,\n    MultilineInput,\n    Output,\n)\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_USER,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-input\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n    minimized = True\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Input Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n            input_types=[],\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n            temp_file=True,\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Chat Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    async def message_response(self) -> Message:\n        background_color = self.background_color\n        text_color = self.text_color\n        icon = self.chat_icon\n\n        message = await Message.create(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n            properties={\n                \"background_color\": background_color,\n                \"text_color\": text_color,\n                \"icon\": icon,\n            },\n        )\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = await self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n"
              },
              "files": {
                "_input_type": "FileInput",
                "advanced": true,
                "display_name": "Files",
                "dynamic": false,
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "file_path": "",
                "info": "Files to be sent with the message.",
                "list": true,
                "list_add_label": "Add More",
                "name": "files",
                "placeholder": "",
                "required": false,
                "show": true,
                "temp_file": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "file",
                "value": ""
              },
              "input_value": {
                "_input_type": "MultilineInput",
                "advanced": false,
                "copy_field": false,
                "display_name": "Input Text",
                "dynamic": false,
                "info": "Message to be passed as input.",
                "input_types": [],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "multiline": true,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "User"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "text_color": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Text Color",
                "dynamic": false,
                "info": "The text color of the name",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "text_color",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": false,
          "type": "ChatInput"
        },
        "dragging": false,
        "id": "ChatInput-70MFi",
        "measured": {
          "height": 48,
          "width": 192
        },
        "position": {
          "x": 396.96334725177803,
          "y": 573.5099149029546
        },
        "selected": false,
        "type": "genericNode"
      },
      {
        "data": {
          "id": "ChatOutput-45RHL",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Display a chat message in the Playground.",
            "display_name": "Chat Output",
            "documentation": "https://docs.langflow.org/components-io#chat-output",
            "edited": false,
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template",
              "background_color",
              "chat_icon",
              "text_color",
              "clean_data"
            ],
            "frozen": false,
            "icon": "MessagesSquare",
            "legacy": false,
            "lf_version": "1.5.0.post2",
            "metadata": {},
            "minimized": true,
            "output_types": [],
            "outputs": [
              {
                "allows_loop": false,
                "cache": true,
                "display_name": "Output Message",
                "group_outputs": false,
                "method": "message_response",
                "name": "message",
                "selected": "Message",
                "tool_mode": true,
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "background_color": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Background Color",
                "dynamic": false,
                "info": "The background color of the icon.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "background_color",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "chat_icon": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Icon",
                "dynamic": false,
                "info": "The icon of the message.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "chat_icon",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "clean_data": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Basic Clean Data",
                "dynamic": false,
                "info": "Whether to clean the data",
                "list": false,
                "list_add_label": "Add More",
                "name": "clean_data",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from collections.abc import Generator\nfrom typing import Any\n\nimport orjson\nfrom fastapi.encoders import jsonable_encoder\n\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.helpers.data import safe_convert\nfrom langflow.inputs.inputs import BoolInput, DropdownInput, HandleInput, MessageTextInput\nfrom langflow.schema.data import Data\nfrom langflow.schema.dataframe import DataFrame\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.template.field.base import Output\nfrom langflow.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    documentation: str = \"https://docs.langflow.org/components-io#chat-output\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n    minimized = True\n\n    inputs = [\n        HandleInput(\n            name=\"input_value\",\n            display_name=\"Inputs\",\n            info=\"Message to be passed as output.\",\n            input_types=[\"Data\", \"DataFrame\", \"Message\"],\n            required=True,\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"clean_data\",\n            display_name=\"Basic Clean Data\",\n            value=True,\n            info=\"Whether to clean the data\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Output Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, id_: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if id_:\n            source_dict[\"id\"] = id_\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            # Handle case where source is a ChatOpenAI object\n            if hasattr(source, \"model_name\"):\n                source_dict[\"source\"] = source.model_name\n            elif hasattr(source, \"model\"):\n                source_dict[\"source\"] = str(source.model)\n            else:\n                source_dict[\"source\"] = str(source)\n        return Source(**source_dict)\n\n    async def message_response(self) -> Message:\n        # First convert the input to string if needed\n        text = self.convert_to_string()\n\n        # Get source properties\n        source, icon, display_name, source_id = self.get_properties_from_source_component()\n        background_color = self.background_color\n        text_color = self.text_color\n        if self.chat_icon:\n            icon = self.chat_icon\n\n        # Create or use existing Message object\n        if isinstance(self.input_value, Message):\n            message = self.input_value\n            # Update message properties\n            message.text = text\n        else:\n            message = Message(text=text)\n\n        # Set message properties\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(source_id, display_name, source)\n        message.properties.icon = icon\n        message.properties.background_color = background_color\n        message.properties.text_color = text_color\n\n        # Store message if needed\n        if self.session_id and self.should_store_message:\n            stored_message = await self.send_message(message)\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n\n    def _serialize_data(self, data: Data) -> str:\n        \"\"\"Serialize Data object to JSON string.\"\"\"\n        # Convert data.data to JSON-serializable format\n        serializable_data = jsonable_encoder(data.data)\n        # Serialize with orjson, enabling pretty printing with indentation\n        json_bytes = orjson.dumps(serializable_data, option=orjson.OPT_INDENT_2)\n        # Convert bytes to string and wrap in Markdown code blocks\n        return \"```json\\n\" + json_bytes.decode(\"utf-8\") + \"\\n```\"\n\n    def _validate_input(self) -> None:\n        \"\"\"Validate the input data and raise ValueError if invalid.\"\"\"\n        if self.input_value is None:\n            msg = \"Input data cannot be None\"\n            raise ValueError(msg)\n        if isinstance(self.input_value, list) and not all(\n            isinstance(item, Message | Data | DataFrame | str) for item in self.input_value\n        ):\n            invalid_types = [\n                type(item).__name__\n                for item in self.input_value\n                if not isinstance(item, Message | Data | DataFrame | str)\n            ]\n            msg = f\"Expected Data or DataFrame or Message or str, got {invalid_types}\"\n            raise TypeError(msg)\n        if not isinstance(\n            self.input_value,\n            Message | Data | DataFrame | str | list | Generator | type(None),\n        ):\n            type_name = type(self.input_value).__name__\n            msg = f\"Expected Data or DataFrame or Message or str, Generator or None, got {type_name}\"\n            raise TypeError(msg)\n\n    def convert_to_string(self) -> str | Generator[Any, None, None]:\n        \"\"\"Convert input data to string with proper error handling.\"\"\"\n        self._validate_input()\n        if isinstance(self.input_value, list):\n            return \"\\n\".join([safe_convert(item, clean_data=self.clean_data) for item in self.input_value])\n        if isinstance(self.input_value, Generator):\n            return self.input_value\n        return safe_convert(self.input_value)\n"
              },
              "data_template": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Data Template",
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "data_template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "{text}"
              },
              "input_value": {
                "_input_type": "HandleInput",
                "advanced": false,
                "display_name": "Inputs",
                "dynamic": false,
                "info": "Message to be passed as output.",
                "input_types": [
                  "Data",
                  "DataFrame",
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "name": "input_value",
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "sender": {
                "_input_type": "DropdownInput",
                "advanced": true,
                "combobox": false,
                "dialog_inputs": {},
                "display_name": "Sender Type",
                "dynamic": false,
                "info": "Type of sender.",
                "name": "sender",
                "options": [
                  "Machine",
                  "User"
                ],
                "options_metadata": [],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "toggle": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "Machine"
              },
              "sender_name": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Sender Name",
                "dynamic": false,
                "info": "Name of the sender.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "sender_name",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": "AI"
              },
              "session_id": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Session ID",
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "session_id",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "should_store_message": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "Store Messages",
                "dynamic": false,
                "info": "Store the message in the history.",
                "list": false,
                "list_add_label": "Add More",
                "name": "should_store_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": true
              },
              "text_color": {
                "_input_type": "MessageTextInput",
                "advanced": true,
                "display_name": "Text Color",
                "dynamic": false,
                "info": "The text color of the name",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "list_add_label": "Add More",
                "load_from_db": false,
                "name": "text_color",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              }
            },
            "tool_mode": false
          },
          "showNode": false,
          "type": "ChatOutput"
        },
        "dragging": false,
        "id": "ChatOutput-45RHL",
        "measured": {
          "height": 48,
          "width": 192
        },
        "position": {
          "x": 1217.989447004333,
          "y": 2309.439423624829
        },
        "selected": false,
        "type": "genericNode"
      }
    ],
    "viewport": {
      "x": -231.51933646311443,
      "y": -333.1703178675482,
      "zoom": 0.7534547426616351
    }
  },
  "description": "Redox potential script preparer and calculator",
  "endpoint_name": null,
  "id": "cfc72f63-b9d3-43b0-9559-220a9343d4ec",
  "is_component": false,
  "last_tested_version": "1.5.0.post2",
  "name": "redoxflow",
  "tags": []
}